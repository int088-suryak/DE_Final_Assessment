# ğŸ“¡ Business Central Data Ingestion Connector

## ğŸ“˜ Overview

This connector extracts data from Microsoft Business Central via API, processes it, and stores it in Azure Synapse. The connector handles API authentication, data preprocessing, and manages the data pipeline to ensure consistent data storage and availability for analytics.

## âœ… Prerequisites

Before using this connector, ensure the following requirements are met:

### ğŸ” Required Credentials:

* **Client ID**: Your Azure AD application client ID
* **Client Secret**: Client secret for Azure AD authentication (stored in Azure Key Vault)
* **Company ID**: Your Microsoft Business Central company ID
* **Company Name**: The name of your Business Central company environment
* **OAuth Token**: Access token with the appropriate scope for the Business Central API

### âš™ï¸ Environment Setup:

* **Python Version**: Python 3.8+
* **PySpark Version**: Latest stable version
* **ODBC Driver**: ODBC Driver 18 for SQL Server
* **Azure Services**:

  * Azure Key Vault configured with necessary secrets
  * Azure Synapse access
  * Azure Storage Blob for storing extracted data

## ğŸ“¥ Data Points for Ingestion

The connector can extract data from any Business Central endpoints configured in the source mapping file. The endpoints are defined in an Excel file located at:
`<your_mapping_file_path>/example_Source.xlsx`

## ğŸ§° Supported Tools & Destinations

### ğŸ› ï¸ Tools Used in This Script

* âœ… Python (PySpark)
* âœ… Azure Synapse

### ğŸ“ Files

* âœ… Parquet

### ğŸ¢ Data Warehouse / Database

* âœ… Azure SQL

## âš™ï¸ Other Supported but Not Used in Script

### ğŸ“ Files

* âœ… JSON
* âœ… CSV
* âœ… ORC

### ğŸŒŠ Lakehouse

* âœ… Databricks: Delta Lake
* âœ… Fabric - Lakehouse

### ğŸ¢ Data Warehouse / Database

* âœ… Snowflake
* âœ… PostgreSQL
* âœ… MySQL
* âœ… Databricks
* âœ… Fabric - Warehouse

## âœ¨ Key Features

1. **Dynamic Schema Inference**: Automatically detects and maps the schema from Business Central API responses
2. **Pagination Support**: Handles paginated responses from the API to fetch complete datasets
3. **Data Type Conversion**: Preprocesses data to ensure correct data types for analytical use
4. **Secure Authentication**: Uses Azure Key Vault for secure credential storage
5. **Error Handling**: Robust error handling to maintain data integrity

## âš ï¸ Limitations:

1. **Rate Limiting**: Business Central API may enforce rate limits that restrict the frequency of data requests
2. **Data Volume**: Very large datasets may require additional optimization for efficient processing
3. **API Dependencies**: Any changes to the Business Central API structure might require connector updates
4. **Authentication Timing**: Access tokens have a limited lifetime and must be refreshed accordingly
5. **Connection Requirements**: Requires proper network connectivity to both Business Central API and Azure services

## ğŸ§ª Usage Example

```bash
# Run the connector with default configuration
python BC_Tables.py
```

## ğŸ§¯ Error Handling

The connector includes basic error handling. You may want to enhance it with:

* Logging to a centralized log repository
* Alerting mechanisms for critical failures
* Retry logic for transient errors
* Transaction management for database operations

### ğŸš€ **Ready to Deploy?**

Follow the setup guide, configure credentials, and start ingesting Business Central data into Azure Synapse seamlessly!

# **References**
* https://learn.microsoft.com/en-us/dynamics365/business-central/dev-itpro/api-reference/v2.0/
